import random
import torch
import tqdm
import transformers
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_liner_schedule_with_warmup
from torch.utils.data import DataLoader, TensorDataset, RandomSampler

#defining device, model and tokenizer
device = torch.device('cuda:0')
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')
model = BertForSequenceClassification.from_pretrained("bert-base-multilingual-uncased", num_labels=2, return_dict=False)
model.eval()
if torch.cuda.is_available():
    model = model.cuda()

#importing finnish data
dfin = pd.read_csv('Suomi.csv') # dataset with 120 000 finnish sentences
dfin.columns=['Text']
dfin["Lang"] = 0
print(dfin)

#importing english data
deng = pd.read_csv("Englanti.csv") # dataset with 120 000 english sentences
deng.columns=['Text']
deng["Lang"] = 1
print(deng)

#combining two datasets
df = pd.concat([dfin, deng], ignore_index=True, sort=False)
print(df)

#splitting dataset to training and validation
trainandval, test = train_test_split(df, test_size=0.15, random_state=42)
train, validation = train_test_split(trainandval, test_size=0.2, random_state=42)

#splitting data to features (text) and labels (1 for english and 0 for finnish)
Xtest = test.iloc[:,0]
Ytest = test.iloc[:,1]

Xval = validation.iloc[:,0]
Yval = validation.iloc[:,1]

Xtrain = train.iloc[:,0]
Ytrain = train.iloc[:,1]

#putting values to list
Xtest = Xtest.values.tolist()
Xval = Xval.values.tolist()
Xtrain = Xtrain.values.tolist()

#making empty lists for tokenization
XtestTokenized = []
XvalTokenized = []
XtrainTokenized =[]

#tokenizers
for sentence in Xtest:
    XtestTokenized.append(tokenizer(sentence, truncation=True, padding='max_length', max_length=512))

for sentence in Xval:
    XvalTokenized.append(tokenizer(sentence, truncation=True, padding='max_length', max_length=512))

for sentence in Xtrain:
    XtrainTokenized.append(tokenizer(sentence, truncation=True, padding='max_length', max_length=512))

#making empty lists for ids and masks
train_seq = []
train_mask =[]
val_seq = []
val_mask = []
test_seq = []
test_mask = []

#splitting data to ids and masks
for i in range(len(XtrainTokenized)):
    train_seq.append(XtrainTokenized[i]['input_ids'])
    train_mask.append(XtrainTokenized[i]['attention_mask'])

for i in range(len(XvalTokenized)):
    val_seq.append(XvalTokenized[i]['input_ids'])
    val_mask.append(XvalTokenized[i]['attention_mask'])

for i in range(len(XtestTokenized)):
    test_seq.append(XtestTokenized[i]['input_ids'])
    test_mask.append(XtestTokenized[i]['attention_mask'])

#tensoring values
train_seq = torch.tensor(train_seq)
train_mask = torch.tensor(train_mask)
Ytrain = torch.tensor(Ytrain.values.tolist())

val_seq = torch.tensor(val_seq)
val_mask = torch.tensor(val_mask)
Yval = torch.tensor(Yval.values.tolist())

test_seq = torch.tensor(test_seq)
test_mask = torch.tensor(test_mask)
Ytest = torch.tensor(Ytest.values.tolist())

#defining batch size
batch_size = 16

#Loading data to DataLoaders
train_data = TensorDataset(train_seq, train_mask, Ytrain)
train_dataloader=DataLoader(
            train_data,
            sampler=RandomSampler(train_data),
            batch_size=batch_size,
            num_workers=0
        )
val_data = TensorDataset(val_seq, val_mask, Yval)
val_dataloader= DataLoader(
            val_data,
            batch_size=batch_size,
            num_workers=0
        )

test_data = TensorDataset(test_seq, test_mask, Ytest)
test_dataloader= DataLoader(
            val_data,
            batch_size=batch_size,
            num_workers=0
        )

#Defining training parameters, optimizer and scheduler
epochs = 6
total_steps = len(train_dataloader) * epochs
steps_per_epoch = len(Xtrain)  // batch_size
total_training_steps = steps_per_epoch * epochs
warmup_steps = total_training_steps // 5

optimizer = AdamW(model.parameters(),
                  lr= 2e-5,
                  correct_bias=False
                  )
scheduler= get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_training_steps
        )
        
#defining evaluation function        
def evaluate(model, dataloader):
    print("\nEvaluating...")
    
    #Putting model to evaluation mode
    model.eval()
    
    #zeroing values
    nb_eval_steps = 0
    eval_loss = 0
    predicted_labels, correct_labels = [],[]
    
    #iterating dataloader
    for step,batch in enumerate(dataloader):
        if step % 100 == 0 and not step == 0:
            print(' Batch {:>5,} of {:>5,}.'.format(step, len(dataloader)))
        batch = tuple(t.to(device) for t in batch)
        sent_id, mask, labels = batch

        with torch.no_grad():
            loss, logits = model(sent_id, attention_mask = mask, labels=labels)
        
        outputs = np.argmax(logits.to('cpu'), axis=1)
        labels= labels.to('cpu').numpy()
        predicted_labels += list(outputs)
        correct_labels +=list(labels)
        nb_eval_steps += 1
        eval_loss += loss.mean().item()
        

    #Calculating loss and creating labels lists
    eval_loss = eval_loss / nb_eval_steps
    correct_labels = np.array(correct_labels)
    predicted_labels = np.array(predicted_labels)

    #returning loss, correct_labels and predictions
    return eval_loss, correct_labels, predicted_labels
    
#defining training parameters    
MODEL_FILE_NAME = "pytorch_model.bin"
GRADIENT_ACCUMULATION_STEPS = 1
MAX_GRAD_NORM = 5
PATIENCE = 3
loss_history = []
no_improvement = 0
 
#Training iterations
for _ in trange(int(epochs), desc="Epoch"):
    model.train()
    total_loss, total_accuracy = 0, 0
    total_preds = []

    for step,batch in enumerate(tqdm(train_dataloader, desc="Training iteration")):
        if step % 100 == 0 and not step == 0:
            print(' Batch {:>5,} of {:>5,}.'.format(step, len(train_dataloader)))

        batch = [r.to(device) for r in batch]

        sent_id, mask, labels = batch
        outputs = model(sent_id, attention_mask=mask, labels=labels)
        loss=outputs[0]

        if GRADIENT_ACCUMULATION_STEPS > 1:
            loss = loss / GRADIENT_ACCUMULATION_STEPS


        total_loss += loss.item()
        loss.backward()

        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)

            optimizer.step()
            optimizer.zero_grad()
            scheduler.step()

    dev_loss, _, _ = evaluate(model, val_dataloader)
    print("Loss history:", loss_history)
    print("Val loss:", dev_loss)

    if len(loss_history) == 0 or dev_loss < min(loss_history):
        no_improvement = 0
        model_to_save= model.module if hasattr(model, 'module') else model
        torch.save(model_to_save.state_dict(), MODEL_FILE_NAME)
    else:
        no_improvement += 1

    if no_improvement >= PATIENCE:
        print("No improvement on developement set. Finish training.")
        break

    loss_history.append(dev_loss) 
